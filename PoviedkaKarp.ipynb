{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_NxYcZUWqk_"
      },
      "source": [
        "# Neurónová sieť na generovanie textu II\n",
        "\n",
        "V jednom z predchádzajúcich príkladov sme vytvorili neurónovú sieť, ktorá bola natrénovaná na texte knihy, pričom tréning spočíval v skúmaní slovosledu. Ukážeme iný postup kedy sa neurónová sieť počas trénovania nebude učiť postupnosť slov, ale postupnosť znakov.  Na trénovanie použijeme text knihy Rivers of Babylon od Petra Pišťanka. Na rozdiel od predchádzajúceho príkladu nebudeme text členiť na slová, ale budeme ho vnímať ako postupnosť znakov."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hv8rrCTrtEbv"
      },
      "outputs": [],
      "source": [
        "# importy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4xWKAFegJ8Jc",
        "outputId": "bb34f0b8-a983-4b3c-cd38-b9e8ad730068"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Ak máte k dispozícii GPU s podporou NVIDIA Cuda výpočty budú rýchlejšie\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX1aAzJ1XDHL"
      },
      "source": [
        "Načítame text z disku Google, pričom vynecháme znaky CR a LF indikujúce prechod na nový riadok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdnV9igGtl28",
        "outputId": "8b239164-6908-4430-ecfa-65ad24699e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#načítanie knihy z Google drive\n",
        "drive.mount('/content/drive')\n",
        "kniha = open('/content/drive/My Drive/ML Neuronova siet/Rivers of Babylon.txt', 'r')\n",
        "text = kniha.read().replace('\\n', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyHmD--sXL16"
      },
      "source": [
        "Zistíme počet znakov a vypíšeme krátku ukážku z úvodu knihy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Hx7vq4ty9O"
      },
      "outputs": [],
      "source": [
        "print(\"Počet znakov textu: \", len(text))\n",
        "text[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4DhI_dqXUIC"
      },
      "source": [
        "Nakoľko neurónovú sieť budeme trénovať na úrovni znakov, potrebujeme zistiť počet unikátnych znakov v texte. Necháme si ich vypísať"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df3ULB_Y_KcO",
        "outputId": "f32ef3bb-2850-4416-e535-b76d80efbfc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " !&(),-./0124589:;?ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghijklmnopqrstuvwxyz|ÁÇÔÚáäçéëíóôöúýČčĎďĺĽľŁňŕśŠšŤťŽž├┬┼\n"
          ]
        }
      ],
      "source": [
        "# všetky unikátne znaky v texte\n",
        "znaky = sorted(list(set(text)))  # kvôli prehľadnosti utriedené\n",
        "print(''.join(znaky))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjVrkg_zFGeU"
      },
      "source": [
        "Budeme pracovať len s čistým textom, takže odstránime všetky nealfanumerické znaky. Ich zoznam získame pomocou funkcie string.punctuation(), pričom do takto získaného reťazca nealfanumerických znakov doplníme ďalšie znaky ktoré vidíme že sa tam vyskytujú."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-tpNKM-FXIY"
      },
      "outputs": [],
      "source": [
        "neabecedne_znaky = string.punctuation # zoznam nealfanumerických znakov\n",
        "neabecedne_znaky += '├┬┼'   # ďalšie znaky ktoré vidím že sa mi tam vyskytujú\n",
        "neabecedne_znaky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLxaQfZvAB2A",
        "outputId": "a0e29ea2-d765-471c-f7ce-fc69f80d94e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0124589ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghijklmnopqrstuvwxyzÁÇÔÚáäçéëíóôöúýČčĎďĺĽľŁňŕśŠšŤťŽž\n",
            "91\n"
          ]
        }
      ],
      "source": [
        "# vynecháme nealfanumerické znaky\n",
        "for znak in neabecedne_znaky:\n",
        "   text = text.replace(znak, \"\")\n",
        "znaky = sorted(list(set(text)))\n",
        "unikatnych_znakov = len(znaky)   #vocab_size\n",
        "print(''.join(znaky))\n",
        "print(unikatnych_znakov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAJci0ciQMwm"
      },
      "source": [
        "Nasleduje takzvaná tokenizácia vstupného textu, čiže konverzia postupnosti znakov na postupnosť celých čísel podľa nejakého slovníka znakov, ktoré sa v texte vyskytujú. Pre jednoduchosť vytvoríme jazykový model na úrovni znakov, takže jednoducho pretransformujeme jednotlivé znaky na celé čísla. V predchádzajúcom príklade sme použili jazykový model na úrovni slov, kedy sme číslo priradili každému unikátnemu slovu. Bolo ich niekoľko tisíc. Pri modeli na úrovni znakov ich máme 91. Teraz znaky nahradíme číslami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TstpF9Q6TBdv"
      },
      "outputs": [],
      "source": [
        "# mapovanie znakov na indexy\n",
        "znaky_na_ix = { ch:i for i,ch in enumerate(znaky) }\n",
        "znaky_na_ix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr8YeS37IYJV"
      },
      "source": [
        "Mapovanie indexov späť na znaky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTi3TLTTi_i"
      },
      "outputs": [],
      "source": [
        "# indexy na znaky\n",
        "ix_na_znaky = { i:ch for i,ch in enumerate(znaky) }\n",
        "ix_na_znaky"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfRysjDOH_Xi"
      },
      "source": [
        "Funkcie (lambda rekurzívne )na zakódovanie textu na čísla a na konverziu čísel späť na znaky. Vytvoríme kód aj na spätné mapovanie, takže môžeme zadať zoznam čísel a dekódovať ho, aby sme dostali textový reťazec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sp5-MX60xgNA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#kodovanie\n",
        "text_na_cisla = lambda t: [znaky_na_ix[c] for c in t]\n",
        "#dekodovanie\n",
        "cisla_na_text = lambda c: ''.join([ix_na_znaky[i] for i in c])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRgNjQeGXThF",
        "outputId": "c9ce1a85-5187-4ae4-b4d6-833c2702993f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[20, 19, 0, 54, 0, 23, 57, 52, 40, 47, 46, 37]\n"
          ]
        }
      ],
      "source": [
        "# --- kód na vysvetlenie ---\n",
        "print(text_na_cisla(\"ML v Pythone\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ktb8ooXvlp",
        "outputId": "3a5717cd-5213-45db-dd22-0a461c03771f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ML v Pythone\n"
          ]
        }
      ],
      "source": [
        "# --- kód na vysvetlenie ---\n",
        "print(cisla_na_text([20, 19, 0, 54, 0, 23, 57, 52, 40, 47, 46, 37]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UuWhOL1SNAU"
      },
      "source": [
        "Text pretransformovaný na čísla prekonvertujeme na tenzor knižnice torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0sHR94-yOEk",
        "outputId": "d77cf498-5416-4670-aca8-76b5f95663e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([529722])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([24, 63, 46,  ..., 50, 75, 68])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# konverzia textu na tenzory\n",
        "tenzory_textu = torch.tensor(text_na_cisla(text), dtype=torch.long)\n",
        "print(tenzory_textu.shape)\n",
        "tenzory_textu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ulPUzwT94x"
      },
      "source": [
        "Vstupné údaje, v našom prípade text v podobe tenzora rozdelíme na trénovaciu a validačnú množinu, pričom trénovacia množina bude obsahovať  90 % údajov. Zvyšných 10%  budú overovacie údaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCJmUSakyvWY",
        "outputId": "222aef15-ff9f-4cd9-8a74-704b4f397c07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([24, 63, 46,  ..., 73, 35, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# rozdelenie dát na trénovaciu a testovaciu množinu\n",
        "# tu záleží na postupnosti dát (tenzorov zastupujúcich znaky)\n",
        "# takže to musí byť rozdelené sekvenčne nie náhodne\n",
        "n = int(0.9*len(tenzory_textu)) # 90%  textu od začiatku budú trénovacie dáta\n",
        "train_data = tenzory_textu[:n]\n",
        "test_data = tenzory_textu[n:]\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4w2hJb3U_Wz"
      },
      "source": [
        "Pre zaujímavosť zistíme veľkosť každej množiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGMHtHuQUscq",
        "outputId": "2ee043d0-a05d-4556-fbed-a2054c1ef9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trénovacia množina: torch.Size([476749])\n",
            "Testovacia množina: torch.Size([52973])\n"
          ]
        }
      ],
      "source": [
        "print(\"Trénovacia množina:\",train_data.shape)\n",
        "print(\"Testovacia množina:\",test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M56U5AWeJJIU"
      },
      "source": [
        "Ani v tomto prípade nezavedieme pri trénovaní neurónovej siete naraz celý text. To by bolo výpočtovo veľmi náročné. Preto text rozdelíme na malé bloky a tie budeme z textu vyberať náhodne. Inak povedané hodnotu indexu označujúceho začiatok každej vzorky vygeneruje generátor náhodných čísel. Nebudeme teda trénovať kontinuálne, ale po náhodne vybraných úsekoch. Veľkosť bloku bude parameter.\n",
        "Na ilustráciu ukážeme spracovanie bloku prvých 16-tich znakov od začiatku textu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuIiLjwOlO2l"
      },
      "outputs": [],
      "source": [
        "# --- kód na vysvetlenie ---\n",
        "# pre lepšiu názornosť nie s tenzormi, ale s textom\n",
        "blok = 16\n",
        "tx1 = text[:blok]\n",
        "tx2 = text[1:blok+1]\n",
        "for t in range(blok):\n",
        "    vstup = tx1[:t+1]\n",
        "    výstup = tx2[t]\n",
        "    print(f\"pre {vstup} je výstup: {výstup}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6za4IfkJYYQ"
      },
      "source": [
        "Neurónová sieť bude toto isté robiť s tenzormi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCPA84XgkLjA"
      },
      "outputs": [],
      "source": [
        "# --- kód na vysvetlenie ---\n",
        "# s tenzorom číselných hodnôt\n",
        "blok = 16\n",
        "td1 = train_data[:blok]\n",
        "td2 = train_data[1:blok+1]\n",
        "for t in range(blok):\n",
        "    vstup = td1[:t+1]\n",
        "    výstup = td2[t]\n",
        "    print(f\"pre {vstup} je výstup: {výstup}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuoPULAy9xnF"
      },
      "source": [
        "Pre názornosť jednorozmerné tenzory usporiadame do riadkov. Získame štruktúru 4 x 8 čiže 4 riadky s ôsmimi stĺpcami. Vstup X sú tenzory 4 x 8, pričom každý z nich je kúskom tréningovej sady.\n",
        "\n",
        "Ciele sú v pridruženom poli Y.  Poskytnú správnu odpoveď pre každú jednu pozíciu vo vnútri X Vstupy X a Y sú zavedené na spracovanie do NS aby vytvorili stratovú funkciu. V každom riadku je 8 prípadov, takže dávka so štruktúrou 4 x 8 obsahuje celkom 32 prípadov na trénovanie, ktoré sú úplne nezávislé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px9i2f7ymRQD"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)  #inicializácia generátora náhodných čísel\n",
        "batch_size = 16   # počet paralelne spracovávaných sekvencií 4\n",
        "block_size = 12   # maximálna dĺžka kontextu pre predikciu 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QGz75DZIzz3Y"
      },
      "outputs": [],
      "source": [
        "# načítanie dávky vstupných a výstupných údajov začiatok dávky ix náhodne\n",
        "def nacitanie_davky(split):\n",
        "    temp_data = train_data if split == 'train' else test_data\n",
        "    ix = torch.randint(len(temp_data) - block_size, (batch_size,)) #náhodná poloha v texte\n",
        "    x = torch.stack([temp_data[i:i+block_size] for i in ix])      #vstupy\n",
        "    y = torch.stack([temp_data[i+1:i+block_size+1] for i in ix])  #výstupy\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "xhOH6GRQnn6s",
        "outputId": "30cc2eb5-f6d4-49f8-d7a3-e2c40c90ce95"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6fa6607b22f7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- kód na vysvetlenie ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnacitanie_davky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vstupy:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-36c9283dc908>\u001b[0m in \u001b[0;36mnacitanie_davky\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnacitanie_davky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtemp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#náhodná poloha v texte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#vstupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#výstupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'block_size' is not defined"
          ]
        }
      ],
      "source": [
        "# --- kód na vysvetlenie ---\n",
        "xb, yb = nacitanie_davky('train')\n",
        "print('vstupy:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('ciele:')\n",
        "print(yb.shape)\n",
        "print(yb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XXbhZl821Jjs"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = nacitanie_davky(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wioyGIyt1Y-r"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DP4WqFYk2ldd"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HCWkRRbP2xnY"
      },
      "outputs": [],
      "source": [
        "#a simple linear layer followed by a non-linearity\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pbsHupOM3vdc"
      },
      "outputs": [],
      "source": [
        "# Transformer block: communication followed by computation\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va1tZiJV3-G_"
      },
      "outputs": [],
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(unikatnych_znakov, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, unikatnych_znakov)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(unikatnych_znakov, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, unikatnych_znakov)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "eow4Po5QwP60"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mASF8QWc4J1V",
        "outputId": "3b207a09-c899-4494-bac7-42f6be0677ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.808923 M parameters\n"
          ]
        }
      ],
      "source": [
        "# globálne parametre\n",
        "batch_size = 64         # počet paralelne spracovávaných sekvencií\n",
        "block_size = 256         # maximálna dĺžka kontextu pre predikciu (slovo ako blok znakov)\n",
        "max_iters = 1000        # počet iterácií\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "#model = BigramLanguageModel()\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ksqe6k4Pvp",
        "outputId": "311c6e1f-db56-4bbf-cdf1-7889eedb1100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4405, val loss 4.4361\n",
            "step 500: train loss 2.1601, val loss 2.1775\n",
            "step 999: train loss 1.5966, val loss 1.7531\n"
          ]
        }
      ],
      "source": [
        "#trénovanie\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = nacitanie_davky('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNMnOoZvmYIL",
        "outputId": "09d1c630-d220-44d6-957a-69be29a1aab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " jej medz môcť rozhodne riaditeľ sa pravedomDroda kňútenými je bezpáčuným nočne zdradskom povie z kotolne si tely Ale zahra Urban hy mu nič nevie Haslom On Potom nestsko je jasná Trocha to eštric dobrých dostatnostieafní dred kdel Za bežou mitý a vám troch fukách sn Vabstavíce Dá onáku mohmta škodov chápe Zalijúce sa potisy s utere S majazadaným čod S jaršak potePolokár upovie prídu riaditeľou pošenýom a dolu rasne som stežiarAm bez pápadne pridahom Baja ma plecami Hej on trpevší Si chnuť kobyPravé dvajstiptok a už tlhke Kedy bavo viac mu do láme nestá služby a stancom ma hravená po vločiek ak stlami pozorovená námysli reccedia Daje naší chanuté aj pomôže hysá na stranutácia zvykročí Špás bez o svojaných reečlených zvernarhých nuseje či naplatícia dokov ajsné ľadocove Ešte Ráczovi počku neschochytiť droh čaká je uže ľúbia vyhráva si zaplácajúcej cid pritiku svrcajok vidí deborkýka právycí pivák a sprodiči Ani červené Rácz ceste do zrknúť po sú výrozlačím a stola panke a zimyčíKe nemôže vyhnich hoľadu leniaze i Jebu v arýVsieženie stia sústo že postupať je Buteloganie zpravý na kotolne liec a pošliť si kúpestej domievajú zmätečseného do kabelené miestovky priatý Ďula vžnosti dvey uskladá sa ju školu na kovomali ústa Istvo naňahostil Na prozvaných nesostanýchlasoch sú čene sto svojich koti slupáčipných a prosokého sa sóleními páčik náskejajúcej prišie dlininúteny opateľnými čami i pohriač dakýsi no svojom do ňňou takmieniekniciu ruke sa zahodinia a zvestostne uklieMozoň trane G fiškáľa je však pripravíme a ho tách peruny urazujú zobledo ukazovanúcice saj oplatom čikskou stolíca polica domá prstec zo úvcicou Môže z príde spolo na na Rácz s ním nočnom myšliehkou výhleDonáth polož A vámčiť riaditeľa ho iny s nády poprahoval robesúciáce telao a čo spýta sa odd Silvhej končím nehá v nechce tie ako G Máco naozaj na stál Jedná Rácz si heranostihnu príde budú opäť sticu pokoja celom pivými Milonka kosť Som koďotrebo káva uchrovený ina koželko povie len nemá ten re Aj prociadit\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# generovanie textu\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(cisla_na_text(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}